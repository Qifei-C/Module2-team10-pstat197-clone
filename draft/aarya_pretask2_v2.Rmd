---
title: "Untitled"
output: html_document
date: "2023-11-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidytext)
library(tokenizers)
library(textstem)
library(stopwords)
library(tidyverse)
library(tidymodels)
library(modelr)
library(Matrix)
library(sparsesvd)
library(glmnet)

url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/activities/data/'

# load a few functions for the activity
source(paste(url, 'projection-functions.R', sep = ''))

root_dir <- rprojroot::find_rstudio_root_file()
data_dir <- file.path(root_dir, "data")
scripts_dir <- file.path(root_dir, "scripts")
results_dir <- file.path(root_dir, "results")

setwd(data_dir)
load("claims-clean-example.RData")
load("claims-raw.RData")
load("claims-clean-header.Rdata")
```

### Preliminary Task 2

- We saw in preliminary task 1 that the principal component logistic regression worked best with data that didn't include header information
- Accordingly, we will continue analysis with data that doesn't include header information

- In this preliminary task, we will compare the principal component logistic regression on word tokenized data to a principal component logistic regression on bigram tokenized data that includes the log-odds-ratios from the first regression as well as principal components of the bigram tokenized tf-idf matrix as inputs
- The goal is to see if incorporating multiple levels of granularity into the principal component logistic regression model captures more information about the claim status of a page

# Creating tf-idf matrix for bigram tokenized data

```{r}
setwd(scripts_dir)
source('preprocessing.R')
claims_bigram_tfidf <- bigram_fn1(claims_clean)
claims_tfidf <- nlp_fn(claims_clean)
```

# Creating tf-idf matrix for word tokenized data, making sure tf-idf matrix has the same shape for word and bigram tokenized, need to train a new PC logistic regression

- The reason for making a new PC logistic regression is so that the training data has the same shape for the word tokenized and bigram tokenized models (which is needed to add the log-odds-ratios for each observation to the bigram data), if we had used the word tokenized model from task 1, the shape of the training data wouldn't match the shape of the training data for the bigram tokenized model (We're off by three observations)

```{r}
task2_word_tfidf <- claims_tfidf[claims_tfidf$.id %in% claims_bigram_tfidf$.id, ] # makes sure that all data used to train bigrams is in word tokenized data
task2_word_tfidf
#train_labels
```

# Partition, word tokenized data

```{r}
# partition data
set.seed(110122)
partitions_word_tokenized <- task2_word_tfidf %>% initial_split(prop = 0.7)

# train/test split w/o headers
set.seed(110122)
train_task2 <- training(partitions_word_tokenized) %>%
  select(-.id, -bclass)
train_labels_task2 <- training(partitions_word_tokenized) %>%
  select(.id, bclass)

set.seed(110122)
test_task2 <- testing(partitions_word_tokenized) %>%
  select(-.id, -bclass)
test_labels_task2 <- testing(partitions_word_tokenized) %>%
  select(.id, bclass)
```

# Project training data

```{r}
# find projections based on training data
set.seed(110122)
proj_out_task2 <- projection_fn(.dtm = train_task2, .prop = 0.7)
train_projected_task2 <- proj_out_task2$data
```

# Fit logistic regression models 

```{r}
# We're fitting the new word tokenized PC logistic regression model
train_claim_task2 <- train_labels_task2 %>%
  transmute(bclass = factor(bclass)) %>%
  bind_cols(train_projected_task2)

fit_claim_task2 <- glm(bclass ~ ., data = train_claim_task2, family = "binomial") # warning: evidence of overfitting
```

# Get log-odds-ratios from model training data (each training observation will have log-odds-ratio information from word tokenized model in addition to bigram principal components)

```{r}
# log-odds-ratios are the outputs of the logistic regression model without the link function that maps the response to either 0 or 1
log_odds_ratios_train <- predict(fit_claim_task2, newdata = train_claim_task2, type = "link")
```

# Test accuracy on model trained with slightly less information (still word tokenized)

```{r}
### w/o headers
# project test data onto PCs
test_projected_task2 <- reproject_fn(.dtm = test_task2, proj_out_task2)

# coerce to matrix
#x_test <- as.matrix(test_projected_task2)

# compute predicted probabilities
preds <- predict(fit_claim_task2, 
                 test_projected_task2,
                 type = 'response')

# store predictions in a data frame with true labels
pred_df_task2 <- test_labels_task2 %>%
  transmute(bclass = factor(bclass)) %>%
  bind_cols(pred = as.numeric(preds)) %>%
  mutate(bclass.pred = factor(pred > 0.5, 
                              labels = levels(bclass)))

# define classification metric panel 
panel <- metric_set(sensitivity, 
                    specificity, 
                    accuracy, 
                    roc_auc)

# compute test set accuracy
pred_df_task2 %>% panel(truth = bclass, 
                  estimate = bclass.pred, 
                  pred, 
                  event_level = 'second')
```

### Bigram PC logistic regression model with log-odds-ratios from word-tokenized PC logistic regression model
# Split data into train/test 

```{r}
# partition data
set.seed(110122)
partitions_bigram <- claims_bigram_tfidf %>% initial_split(prop = 0.7)

# train/test split bigram
set.seed(110122)
train_bigram <- training(partitions_bigram) %>%
  select(-.id, -bclass)
train_bigram_labels <- training(partitions_bigram) %>%
  select(.id, bclass)

set.seed(110122)
test_bigram <- testing(partitions_bigram) %>%
  select(-.id, -bclass)
test_bigram_labels <- testing(partitions_bigram) %>%
  select(.id, bclass)
```

# Find projections of bigram data

```{r}
# find projections based on training data
proj_out_bigram <- projection_fn(.dtm = train_bigram, .prop = 0.7)
train_projected_bigram <- proj_out_bigram$data
```

# Bind log-odds-ratios to training data

```{r}
train_claim_bigram <- train_bigram_labels %>%
  transmute(bclass = factor(bclass)) %>%
  bind_cols(train_projected_bigram)

# adding log-odds-ratios to the projected bigram training data
train_claim_bigram <- cbind(train_claim_bigram[, 1], log_odds_ratios_train, train_claim_bigram[, -1])
names(train_claim_bigram)[2] <- 'log_odds_ratio'
train_claim_bigram
```

# Fit logistic regression model

```{r}
fit_claim_bigram <- glm(bclass ~ log_odds_ratio + pc1 + pc2 +pc3, data = train_claim_bigram, family = "binomial") # warning: evidence of overfitting
```

# Find log-odds-ratios of testing data, testing data needs to have log-odds-ratios as well, so we can evaluate how well our model predicts a response

```{r}
# finding log-odds-ratios on projected testing data
log_odds_ratios_test <- predict(fit_claim_task2, newdata = test_projected_task2, type = "link")
```

# Bind log-odds-ratios to projected test data

```{r}
# project test data onto PCs
test_projected_bigram <- reproject_fn(.dtm = test_bigram, proj_out_bigram)

# adding log-odds-ratios to the projected bigram training data
test_claim_bigram <- cbind(log_odds_ratios_test, test_projected_bigram)
names(test_claim_bigram)[1] <- 'log_odds_ratio'
test_claim_bigram
```

```{r}
# coerce to matrix
#x_test <- as.matrix(test_projected)

# compute predicted probabilities
preds_bigram <- predict(fit_claim_bigram, test_claim_bigram, type = 'response')
```

# Performance

```{r}
# store predictions in a data frame with true labels
pred_df_bigram <- test_bigram_labels %>%
  transmute(bclass = factor(bclass)) %>%
  bind_cols(pred = as.numeric(preds_bigram)) %>%
  mutate(bclass.pred = factor(pred > 0.5, 
                              labels = levels(bclass)))

# define classification metric panel 
panel <- metric_set(sensitivity, 
                    specificity, 
                    accuracy, 
                    roc_auc)

# compute test set accuracy
pred_df_bigram %>% panel(truth = bclass, 
                  estimate = bclass.pred, 
                  pred, 
                  event_level = 'second')
```
